{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce6b97c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\MUSKAN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\gutenberg.zip.\n"
     ]
    }
   ],
   "source": [
    "## data collection\n",
    "import nltk\n",
    "nltk.download('gutenberg')\n",
    "from nltk.corpus import gutenberg\n",
    "import pandas as pd\n",
    "\n",
    "##load dataset\n",
    "data=gutenberg.raw('shakespeare-hamlet.txt')\n",
    "\n",
    "##save file\n",
    "with open('hamlet.txt','w')as file:\n",
    "    file.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12df355",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4818"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##data preprocessing\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "##load dataset\n",
    "with open('hamlet.txt','r') as file:\n",
    "    text=file.read().lower()\n",
    "\n",
    "##tokenize\n",
    "tokenizer=Tokenizer()\n",
    "tokenizer.fit_on_texts([text])      #Splits it into words (tokens),Builds a vocabulary of unique words,Assigns each word a unique integer index.\n",
    "total_words=len(tokenizer.word_index)+1\n",
    "total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5f12d39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This generates all possible prefix n-gram sequences from the line: For [2, 5, 9], it produces:[2, 5] [2, 5, 9]\\n These are sequences where the model could learn: \"Given [2, 5], predict the next word'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##create input sequences\n",
    "input_sequences=[]\n",
    "for line in text.split('\\n'):\n",
    "    token_list=tokenizer.texts_to_sequences([line])[0]   #Example: \"The cat sat\" → [2, 5, 9]\n",
    "    for i in range(1,len(token_list)):\n",
    "        n_gram_sequence=token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)      \n",
    "'''This generates all possible prefix n-gram sequences from the line: For [2, 5, 9], it produces:[2, 5] [2, 5, 9]\n",
    " These are sequences where the model could learn: \"Given [2, 5], predict the next word'''\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30dcc295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##pad sequence\n",
    "max_sequence_len=max([len(x)for x in input_sequences])\n",
    "max_sequence_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7284f0dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,    0,    1,  687],\n",
       "       [   0,    0,    0, ...,    1,  687,    4],\n",
       "       [   0,    0,    0, ...,  687,    4,   45],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,    4,   45, 1047],\n",
       "       [   0,    0,    0, ...,   45, 1047,    4],\n",
       "       [   0,    0,    0, ..., 1047,    4,  193]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sequences=np.array(pad_sequences(input_sequences,maxlen=max_sequence_len,padding='pre'))\n",
    "input_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc3458fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "###create predictor and label\n",
    "import tensorflow as tf\n",
    "x,y=input_sequences[:,:-1],input_sequences[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d681b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=tf.keras.utils.to_categorical(y,num_classes=total_words) ##onehot encoded while multi class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f4917cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into train n test split\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6d8215c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define early stopping\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stopping=EarlyStopping(monitor='val_loss',patience=3,restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a94187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 13, 100)           481800    \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 13, 150)           150600    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 13, 150)           0         \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 100)               100400    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4818)              486618    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1219418 (4.65 MB)\n",
      "Trainable params: 1219418 (4.65 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "##train LSTM RNN\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding,LSTM,Dense,Dropout,GRU\n",
    "\n",
    "##define model\n",
    "model=Sequential()\n",
    "model.add(Embedding(total_words,100,input_length=max_sequence_len-1))\n",
    "model.add(LSTM(150,return_sequences=True))\n",
    "model.add(Dropout(0.2))  #Randomly \"drops\" 20% of the outputs from the previous LSTM layer during training. Prevents overfitting by ensuring the model doesn’t rely too much on any one node.\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(total_words,activation=\"softmax\"))\n",
    "\n",
    "##compile model\n",
    "model.compile(loss=\"categorical_crossentropy\",optimizer=\"adam\",metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cfe194f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "644/644 [==============================] - 17s 26ms/step - loss: 6.2350 - accuracy: 0.0479 - val_loss: 7.0757 - val_accuracy: 0.0433\n",
      "Epoch 2/30\n",
      "644/644 [==============================] - 16s 25ms/step - loss: 6.0855 - accuracy: 0.0530 - val_loss: 7.1748 - val_accuracy: 0.0449\n",
      "Epoch 3/30\n",
      "644/644 [==============================] - 18s 28ms/step - loss: 5.9622 - accuracy: 0.0545 - val_loss: 7.1957 - val_accuracy: 0.0517\n",
      "Epoch 4/30\n",
      "644/644 [==============================] - 17s 26ms/step - loss: 5.8431 - accuracy: 0.0603 - val_loss: 7.2601 - val_accuracy: 0.0552\n"
     ]
    }
   ],
   "source": [
    "##train model\n",
    "history=model.fit(x_train,y_train,epochs=30,validation_data=(x_test,y_test),verbose=1,callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "64f95e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_word(model, tokenizer, text, max_sequence_len):\n",
    "    # Convert the input text into a sequence of tokens (integers)\n",
    "    token_list = tokenizer.texts_to_sequences(text)[0]\n",
    "    \n",
    "    # If the sequence is longer than the max_sequence_len, truncate it\n",
    "    if len(token_list) >= max_sequence_len:\n",
    "        token_list = token_list[-(max_sequence_len-1):]\n",
    "    \n",
    "    # Pad the sequence to ensure it's the right length for the model\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "    \n",
    "    # Get the model's prediction for the next word (probabilities for each word)\n",
    "    predicted = model.predict(token_list, verbose=1)\n",
    "    \n",
    "    # Find the index of the word with the highest predicted probability\n",
    "    predicted_word_index = np.argmax(predicted, axis=1)\n",
    "    \n",
    "    # Look up the word corresponding to the predicted index\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == predicted_word_index:\n",
    "            return word  # Return the predicted word\n",
    "    \n",
    "    # If no word is found, return None\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4295da77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text:Barn. Last night of all,When yond same the\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Next Word PRediction:the\n"
     ]
    }
   ],
   "source": [
    "input_text=\"Barn. Last night of all,When yond same the\"\n",
    "print(f\"Input text:{input_text}\")\n",
    "max_sequence_len=model.input_shape[1]+1\n",
    "next_word=predict_next_word(model,tokenizer,input_text,max_sequence_len)\n",
    "print(f\"Next Word PRediction:{next_word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7237af3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MUSKAN\\data_science_muskan\\RNN\\venv\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "## Save the model\n",
    "model.save(\"next_word_lstm.h5\")\n",
    "## Save the tokenizer\n",
    "import pickle\n",
    "with open('tokenizer.pickle','wb') as handle:\n",
    "    pickle.dump(tokenizer,handle,protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372bd086",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
